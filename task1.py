# -*- coding: utf-8 -*-
"""task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GGRcwSHMPrJrUPqmxxgJI2seW6VY9QoV
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# Configure plotting
plt.style.use('default')
sns.set_palette("husl")

print("=== AI Developer Productivity Data Cleaning & Preprocessing ===\n")

# Step 1: Import and Explore the Dataset
print("Step 1: Loading and Exploring Dataset")
print("-" * 40)

# Load the dataset
df = pd.read_csv('ai_dev_productivity.csv')

# Basic dataset information
print("Dataset Shape:", df.shape)
print("\nDataset Info:")
print(df.info())
print("\nFirst 5 rows:")
print(df.head())
print("\nDataset Description:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())
print(f"\nTotal missing values: {df.isnull().sum().sum()}")

# Check data types
print("\nData Types:")
print(df.dtypes)

# Step 2: Handle Missing Values
print("\n" + "="*50)
print("Step 2: Handling Missing Values")
print("-" * 40)

# Check for any missing values and handle them
missing_values = df.isnull().sum()
if missing_values.sum() > 0:
    print("Found missing values:")
    print(missing_values[missing_values > 0])

    # Handle missing values for numerical columns
    numerical_cols = df.select_dtypes(include=[np.number]).columns
    for col in numerical_cols:
        if df[col].isnull().sum() > 0:
            median_val = df[col].median()
            df[col].fillna(median_val, inplace=True)
            print(f"Filled missing values in {col} with median: {median_val}")

    # Handle missing values for categorical columns
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        if df[col].isnull().sum() > 0:
            mode_val = df[col].mode()[0]
            df[col].fillna(mode_val, inplace=True)
            print(f"Filled missing values in {col} with mode: {mode_val}")
else:
    print("No missing values found in the dataset!")

# Step 3: Handle Categorical Features (Encoding)
print("\n" + "="*50)
print("Step 3: Encoding Categorical Features")
print("-" * 40)

# Identify categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
print(f"Categorical columns found: {categorical_columns}")

if len(categorical_columns) > 0:
    # Apply Label Encoding for categorical features
    label_encoders = {}
    for col in categorical_columns:
        le = LabelEncoder()
        df[col + '_encoded'] = le.fit_transform(df[col])
        label_encoders[col] = le
        print(f"Encoded {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}")

    # Drop original categorical columns
    df_encoded = df.drop(columns=categorical_columns)
else:
    print("No categorical columns found - all features are already numerical!")
    df_encoded = df.copy()

print(f"\nDataset shape after encoding: {df_encoded.shape}")

# Step 4: Detect and Visualize Outliers
print("\n" + "="*50)
print("Step 4: Outlier Detection and Visualization")
print("-" * 40)

# Create boxplots for all numerical features
numerical_features = df_encoded.select_dtypes(include=[np.number]).columns
n_features = len(numerical_features)
n_cols = 3
n_rows = (n_features + n_cols - 1) // n_cols

plt.figure(figsize=(15, 5 * n_rows))
for i, feature in enumerate(numerical_features):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.boxplot(y=df_encoded[feature])
    plt.title(f'Boxplot of {feature}')
    plt.ylabel(feature)

plt.tight_layout()
plt.savefig('outlier_detection_boxplots.png', dpi=300, bbox_inches='tight')
plt.show()

# Detect outliers using IQR method
def detect_outliers_iqr(df, feature):
    Q1 = df[feature].quantile(0.25)
    Q3 = df[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]
    return outliers, lower_bound, upper_bound

print("Outlier Analysis:")
outlier_info = {}
for feature in numerical_features:
    outliers, lower, upper = detect_outliers_iqr(df_encoded, feature)
    outlier_count = len(outliers)
    outlier_info[feature] = {
        'count': outlier_count,
        'percentage': (outlier_count / len(df_encoded)) * 100,
        'lower_bound': lower,
        'upper_bound': upper
    }
    print(f"{feature}: {outlier_count} outliers ({outlier_info[feature]['percentage']:.2f}%)")

# Step 5: Remove Outliers (Conservative approach)
print("\n" + "="*50)
print("Step 5: Removing Outliers")
print("-" * 40)

print(f"Original dataset size: {len(df_encoded)}")

# Remove outliers for features with reasonable outlier counts (< 10% of data)
df_clean = df_encoded.copy()
removed_outliers = 0

for feature in numerical_features:
    if outlier_info[feature]['percentage'] < 10:  # Only remove if outliers < 10%
        outliers, lower, upper = detect_outliers_iqr(df_clean, feature)
        initial_size = len(df_clean)
        df_clean = df_clean[(df_clean[feature] >= lower) & (df_clean[feature] <= upper)]
        removed = initial_size - len(df_clean)
        removed_outliers += removed
        print(f"Removed {removed} outliers from {feature}")
    else:
        print(f"Kept outliers in {feature} (too many: {outlier_info[feature]['percentage']:.2f}%)")

print(f"\nFinal dataset size: {len(df_clean)}")
print(f"Total outliers removed: {removed_outliers}")

# Step 6: Feature Scaling/Normalization
print("\n" + "="*50)
print("Step 6: Feature Scaling and Normalization")
print("-" * 40)

# Separate features and target (assuming task_success is the target)
if 'task_success' in df_clean.columns:
    target_col = 'task_success'
    feature_cols = [col for col in df_clean.columns if col != target_col]

    X = df_clean[feature_cols]
    y = df_clean[target_col]

    print(f"Features: {feature_cols}")
    print(f"Target: {target_col}")
else:
    # If no clear target, treat all as features
    X = df_clean.copy()
    y = None
    feature_cols = X.columns.tolist()
    print("No target variable identified. Scaling all features.")

# Apply StandardScaler
scaler = StandardScaler()
X_scaled = pd.DataFrame(
    scaler.fit_transform(X),
    columns=X.columns,
    index=X.index
)

print("\nFeature scaling completed using StandardScaler")
print("Original feature statistics:")
print(X.describe().round(3))
print("\nScaled feature statistics:")
print(X_scaled.describe().round(3))

# Step 7: Final Data Visualization
print("\n" + "="*50)
print("Step 7: Final Data Visualization")
print("-" * 40)

# Create correlation heatmap
plt.figure(figsize=(12, 10))
correlation_matrix = X_scaled.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('Feature Correlation Heatmap (After Preprocessing)')
plt.tight_layout()
plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()

# Distribution plots after preprocessing
plt.figure(figsize=(15, 10))
for i, feature in enumerate(X_scaled.columns):
    plt.subplot(3, 3, i + 1)
    sns.histplot(X_scaled[feature], kde=True)
    plt.title(f'Distribution of {feature} (Scaled)')
    plt.xlabel(feature)

plt.tight_layout()
plt.savefig('feature_distributions_scaled.png', dpi=300, bbox_inches='tight')
plt.show()

# Step 8: Save Cleaned Dataset
print("\n" + "="*50)
print("Step 8: Saving Cleaned Dataset")
print("-" * 40)

# Save the cleaned and preprocessed dataset
if y is not None:
    final_dataset = X_scaled.copy()
    final_dataset[target_col] = y
else:
    final_dataset = X_scaled.copy()

final_dataset.to_csv('ai_dev_productivity_cleaned.csv', index=False)
print("Cleaned dataset saved as 'ai_dev_productivity_cleaned.csv'")

# Summary Report
print("\n" + "="*60)
print("PREPROCESSING SUMMARY REPORT")
print("="*60)
print(f"Original dataset shape: {df.shape}")
print(f"Final dataset shape: {final_dataset.shape}")
print(f"Features processed: {len(feature_cols)}")
print(f"Missing values handled: {missing_values.sum()}")
print(f"Outliers removed: {removed_outliers}")
print(f"Categorical features encoded: {len(categorical_columns)}")
print(f"Features standardized: {len(X_scaled.columns)}")

print("\nData preprocessing completed successfully!")
print("Files generated:")
print("- ai_dev_productivity_cleaned.csv (cleaned dataset)")
print("- outlier_detection_boxplots.png (outlier visualization)")
print("- correlation_heatmap.png (feature correlations)")
print("- feature_distributions_scaled.png (scaled feature distributions)")

# Display final dataset info
print("\nFinal Dataset Info:")
print(final_dataset.info())
print("\nFinal Dataset Sample:")
print(final_dataset.head())

